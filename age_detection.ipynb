{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "VwnF38WkYqa9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import PIL\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NHGxc3Uo6PA",
        "outputId": "22f3df34-e0da-405a-ea10-228d6a705da3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/'"
      ],
      "metadata": {
        "id": "Azxk2kw_qLxI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = {}\n",
        "def count_files(start_path):\n",
        "  counts[start_path] = 0\n",
        "  for name in os.listdir(start_path):\n",
        "    if os.path.isdir(f'{start_path}{name}/'):\n",
        "      count_files(f'{start_path}{name}/')\n",
        "    else:\n",
        "      counts[start_path] += 1\n",
        "\n",
        "count_files(data_path)"
      ],
      "metadata": {
        "id": "U4c7EgOOs1MR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del transforms"
      ],
      "metadata": {
        "id": "roxf9gqSHvjr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pbu1mU0uaFb",
        "outputId": "18c45f02-e25e-47bd-c400-1aed1f7295c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/': 0,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/0-2/': 1605,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/13-18/': 1082,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/19-24/': 2408,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/25-35/': 8204,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/3-12/': 1808,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/36-44/': 2548,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/45-60/': 3656,\n",
              " '/content/drive/MyDrive/Colab Notebooks/age_detection/data/data/60+/': 2397}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomAutocontrast(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "dataset = datasets.ImageFolder(data_path, transform=dataset_transforms)"
      ],
      "metadata": {
        "id": "6dfFJHeNFAgW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Link PDF: https://drive.google.com/drive/u/0/folders/1MZ9lhzGKABXLMTJzeIM2b-zi7YJnn6WR\n",
        "* Link Chess: https://github.com/LeventSoykan/Chess_Piece_Image_Classification_With_CNN/blob/main/image_classification.ipynb\n",
        "* Dataset: https://susanqq.github.io/UTKFace/\n"
      ],
      "metadata": {
        "id": "lDwabnjfrdQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU Batch Size of 32 used, and random seeds prepared for consistency among splitting & training processes\n",
        "validation_split = .2\n",
        "test_split = .3\n",
        "batch_size = 32\n",
        "seed_val = 1903\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "ZRZiXqX9FJUR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random shuffling indices before split to get similar distribution among classes\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "np.random.shuffle(indices)"
      ],
      "metadata": {
        "id": "1ExrkdoKFRmV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_test = int(np.floor(test_split * dataset_size))\n",
        "split_val = int(np.floor(validation_split * (dataset_size-split_test)))+split_test\n",
        "split_test, split_val, dataset_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amGSDqHlFd7E",
        "outputId": "d4d1526a-8de3-4a2f-87bb-f1681640951e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7112, 10431, 23708)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Train-Test-Validation Datasets\n",
        "train_indices, val_indices, test_indices = indices[split_val:], indices[split_test:split_val], indices[:split_test]"
      ],
      "metadata": {
        "id": "IwigDBHVFnCt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, drop_last=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, drop_last=True)"
      ],
      "metadata": {
        "id": "GZuCczX0Fpxw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomAutocontrast(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset_train = datasets.ImageFolder(\n",
        "    data_path_train,\n",
        "    transform = train_transforms\n",
        ")\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset_test = datasets.ImageFolder(\n",
        "    data_path_test,\n",
        "    transform = test_transforms\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Wp73QThorfpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "equ7SzZaFo1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBfVGbDpLmT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_train = DataLoader(\n",
        "    dataset_train,\n",
        "    shuffle = True,\n",
        "    batch_size = 1\n",
        ")\n",
        "\n",
        "image, label = next(iter(dataloader_train))\n",
        "print(image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8aSSUlpLie7",
        "outputId": "6a26d55b-9461-47ed-9739-2a2ca66dbb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 128, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = image.squeeze().permute(1, 2, 0)\n",
        "print(image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH160fKFL_0i",
        "outputId": "d0d868fc-866d-4149-9f8d-5537306fb389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 128, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "-eSn1xztIgEj",
        "outputId": "231f8740-7acd-445a-fd31-cd097c755b91"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ImageFolder' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a62c974a6759>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgeDetectNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Flatten()\n",
        "    )\n",
        "    self.classifier = nn.Linear(16*32*32, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature_extractor(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hASjjDBMNPS4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = AgeDetectNet(num_classes=8)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "xORIWvPvpYGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b16a4a5-5730-4653-cf90-b5e09f309e2f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgeDetectNet(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (classifier): Linear(in_features=16384, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "AjQys_zApb-K"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 2\n",
        "\n",
        "device = 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    train_accuracy = 0.0\n",
        "    valid_accuracy = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    i=0\n",
        "    for images, labels in train_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate batch accuracy\n",
        "        probs = nn.functional.softmax(output, dim=1)\n",
        "        preds = torch.argmax(probs, axis=1)\n",
        "        accuracy = (abs(preds-labels)==0).type(torch.float).mean()\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, labels)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss & accuracy\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "        train_accuracy += accuracy*images.size(0)\n",
        "\n",
        "        #inform user about elapsed time in every 320 batches (320x32=10240 images)\n",
        "        i += 1\n",
        "        if i % 320 == 0:\n",
        "            t1 = time.time()\n",
        "            print(f'{i*images.shape[0]} items processed in {format_time(t1-t0)}')\n",
        "\n",
        "\n",
        "    ######################\n",
        "    # Validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for images, labels in val_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(images)\n",
        "        # calculate batch accuracy\n",
        "        probs = nn.functional.softmax(output, dim=1)\n",
        "        preds = torch.argmax(probs, axis=1)\n",
        "        accuracy = (abs(preds-labels)==0).type(torch.float).mean()\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, labels)\n",
        "        # update average validation loss & accuracy\n",
        "        valid_loss += loss.item()*images.size(0)\n",
        "        valid_accuracy += accuracy*images.size(0)\n",
        "\n",
        "    # calculate average losses & accuracies\n",
        "    train_loss = train_loss/len(train_dataloader.sampler)\n",
        "    valid_loss = valid_loss/len(val_dataloader.sampler)\n",
        "    train_accuracy = train_accuracy/len(train_dataloader.sampler)\n",
        "    valid_accuracy = valid_accuracy/len(val_dataloader.sampler)\n",
        "\n",
        "    # print training/testing statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    print('Epoch: {} \\tTraining Accuracy: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
        "        epoch, train_accuracy, valid_accuracy))\n",
        "\n",
        "    # save model if testing loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_chess.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "metadata": {
        "id": "Fu8P-UzPpcxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(num_classes=5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "  for images,labels in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "C8Oq-d0ZGtqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F33Q-v_-pXQJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}